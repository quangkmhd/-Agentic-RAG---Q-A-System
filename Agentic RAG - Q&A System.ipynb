{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install pandas langchain langchain-community sentence-transformers faiss-cpu smolagents --upgrade -q"],"metadata":{"id":"BMixoUgRyO8F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"],"metadata":{"id":"o839zqxbyRrr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datasets\n","\n","knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"],"metadata":{"id":"9wU3cyc0yV3r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","from transformers import AutoTokenizer\n","from langchain.docstore.document import Document\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.vectorstores import FAISS\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","from langchain_community.vectorstores.utils import DistanceStrategy\n","\n","source_docs = [\n","    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]})\n","    for doc in knowledge_base\n","]\n","\n","text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n","    AutoTokenizer.from_pretrained(\"thenlper/gte-small\"),\n","    chunk_size=200,\n","    chunk_overlap=20,\n","    add_start_index=True,\n","    strip_whitespace=True,\n","    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",")\n","\n","# Split docs and keep only unique ones\n","print(\"Splitting documents...\")\n","docs_processed = []\n","unique_texts = {}\n","for doc in tqdm(source_docs):\n","    new_docs = text_splitter.split_documents([doc])\n","    for new_doc in new_docs:\n","        if new_doc.page_content not in unique_texts:\n","            unique_texts[new_doc.page_content] = True\n","            docs_processed.append(new_doc)\n","\n","print(\n","    \"Embedding documents... This should take a few minutes (5 minutes on MacBook with M1 Pro)\"\n",")\n","embedding_model = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n","vectordb = FAISS.from_documents(\n","    documents=docs_processed,\n","    embedding=embedding_model,\n","    distance_strategy=DistanceStrategy.COSINE,\n",")"],"metadata":{"id":"okn1CYYDyXTj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from smolagents import Tool\n","from langchain_core.vectorstores import VectorStore\n","\n","\n","class RetrieverTool(Tool):\n","    name = \"retriever\"\n","    description = \"Using semantic similarity, retrieves some documents from the knowledge base that have the closest embeddings to the input query.\"\n","    inputs = {\n","        \"query\": {\n","            \"type\": \"string\",\n","            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n","        }\n","    }\n","    output_type = \"string\"\n","\n","    def __init__(self, vectordb: VectorStore, **kwargs):\n","        super().__init__(**kwargs)\n","        self.vectordb = vectordb\n","\n","    def forward(self, query: str) -> str:\n","        assert isinstance(query, str), \"Your search query must be a string\"\n","\n","        docs = self.vectordb.similarity_search(\n","            query,\n","            k=7,\n","        )\n","\n","        return \"\\nRetrieved documents:\\n\" + \"\".join(\n","            [\n","                f\"===== Document {str(i)} =====\\n\" + doc.page_content\n","                for i, doc in enumerate(docs)\n","            ]\n","        )"],"metadata":{"id":"nJchUL_DyZFE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from smolagents import HfApiModel, ToolCallingAgent\n","\n","model = HfApiModel(\"meta-llama/Llama-3.1-70B-Instruct\")\n","\n","retriever_tool = RetrieverTool(vectordb)\n","agent = ToolCallingAgent(\n","    tools=[retriever_tool], model=model\n",")"],"metadata":{"id":"Rwaw10x7ya7T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["agent_output = agent.run(\"How can I push a model to the Hub?\")\n","\n","print(\"Final output:\")\n","print(agent_output)"],"metadata":{"id":"sF363FaVycTT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")"],"metadata":{"id":"eWvaZcI7yeez"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import logging\n","\n","agent.logger.setLevel(logging.WARNING) # Let's reduce the agent's verbosity level\n","\n","eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")"],"metadata":{"id":"Rl0LBC4Syf8D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs_agentic_rag = []\n","\n","for example in tqdm(eval_dataset):\n","    question = example[\"question\"]\n","\n","    enhanced_question = f\"\"\"Using the information contained in your knowledge base, which you can access with the 'retriever' tool,\n","give a comprehensive answer to the question below.\n","Respond only to the question asked, response should be concise and relevant to the question.\n","If you cannot find information, do not give up and try calling your retriever again with different arguments!\n","Make sure to have covered the question completely by calling the retriever tool several times with semantically different queries.\n","Your queries should not be questions but affirmative form sentences: e.g. rather than \"How do I load a model from the Hub in bf16?\", query should be \"load a model from the Hub bf16 weights\".\n","\n","Question:\n","{question}\"\"\"\n","    answer = agent.run(enhanced_question)\n","    print(\"=======================================================\")\n","    print(f\"Question: {question}\")\n","    print(f\"Answer: {answer}\")\n","    print(f'True answer: {example[\"answer\"]}')\n","\n","    results_agentic = {\n","        \"question\": question,\n","        \"true_answer\": example[\"answer\"],\n","        \"source_doc\": example[\"source_doc\"],\n","        \"generated_answer\": answer,\n","    }\n","    outputs_agentic_rag.append(results_agentic)"],"metadata":{"id":"RTuBGfkjyhNT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from huggingface_hub import InferenceClient\n","\n","reader_llm = InferenceClient(\"Qwen/Qwen2.5-72B-Instruct\")\n","\n","outputs_standard_rag = []\n","\n","for example in tqdm(eval_dataset):\n","    question = example[\"question\"]\n","    context = retriever_tool(question)\n","\n","    prompt = f\"\"\"Given the question and supporting documents below, give a comprehensive answer to the question.\n","Respond only to the question asked, response should be concise and relevant to the question.\n","Provide the number of the source document when relevant.\n","If you cannot find information, do not give up and try calling your retriever again with different arguments!\n","\n","Question:\n","{question}\n","\n","{context}\n","\"\"\"\n","    messages = [{\"role\": \"user\", \"content\": prompt}]\n","    answer = reader_llm.chat_completion(messages).choices[0].message.content\n","\n","    print(\"=======================================================\")\n","    print(f\"Question: {question}\")\n","    print(f\"Answer: {answer}\")\n","    print(f'True answer: {example[\"answer\"]}')\n","\n","    results_agentic = {\n","        \"question\": question,\n","        \"true_answer\": example[\"answer\"],\n","        \"source_doc\": example[\"source_doc\"],\n","        \"generated_answer\": answer,\n","    }\n","    outputs_standard_rag.append(results_agentic)"],"metadata":{"id":"LqKcPHJ3yiuz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EVALUATION_PROMPT = \"\"\"You are a fair evaluator language model.\n","\n","You will be given an instruction, a response to evaluate, a reference answer that gets a score of 3, and a score rubric representing a evaluation criteria are given.\n","1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n","2. After writing a feedback, write a score that is an integer between 1 and 3. You should refer to the score rubric.\n","3. The output format should look as follows: \\\"Feedback: {{write a feedback for criteria}} [RESULT] {{an integer number between 1 and 3}}\\\"\n","4. Please do not generate any other opening, closing, and explanations. Be sure to include [RESULT] in your output.\n","5. Do not score conciseness: a correct answer that covers the question should receive max score, even if it contains additional useless information.\n","\n","The instruction to evaluate:\n","{instruction}\n","\n","Response to evaluate:\n","{response}\n","\n","Reference Answer (Score 3):\n","{reference_answer}\n","\n","Score Rubrics:\n","[Is the response complete, accurate, and factual based on the reference answer?]\n","Score 1: The response is completely incomplete, inaccurate, and/or not factual.\n","Score 2: The response is somewhat complete, accurate, and/or factual.\n","Score 3: The response is completely complete, accurate, and/or factual.\n","\n","Feedback:\"\"\""],"metadata":{"id":"1LtwMnlFykCc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from huggingface_hub import InferenceClient\n","\n","evaluation_client = InferenceClient(\"meta-llama/Llama-3.1-70B-Instruct\")"],"metadata":{"id":"DoQKgGCSylxb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","results = {}\n","for system_type, outputs in [\n","    (\"agentic\", outputs_agentic_rag),\n","    (\"standard\", outputs_standard_rag),\n","]:\n","    for experiment in tqdm(outputs):\n","        eval_prompt = EVALUATION_PROMPT.format(\n","            instruction=experiment[\"question\"],\n","            response=experiment[\"generated_answer\"],\n","            reference_answer=experiment[\"true_answer\"],\n","        )\n","        messages = [\n","            {\"role\": \"system\", \"content\": \"You are a fair evaluator language model.\"},\n","            {\"role\": \"user\", \"content\": eval_prompt},\n","        ]\n","\n","        eval_result = evaluation_client.text_generation(\n","            eval_prompt, max_new_tokens=1000\n","        )\n","        try:\n","            feedback, score = [item.strip() for item in eval_result.split(\"[RESULT]\")]\n","            experiment[\"eval_score_LLM_judge\"] = score\n","            experiment[\"eval_feedback_LLM_judge\"] = feedback\n","        except:\n","            print(f\"Parsing failed - output was: {eval_result}\")\n","\n","    results[system_type] = pd.DataFrame.from_dict(outputs)\n","    results[system_type] = results[system_type].loc[~results[system_type][\"generated_answer\"].str.contains(\"Error\")]"],"metadata":{"id":"S-md_Y2vynHi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DEFAULT_SCORE = 2 # Give average score whenever scoring fails\n","def fill_score(x):\n","    try:\n","        return int(x)\n","    except:\n","        return DEFAULT_SCORE\n","\n","for system_type, outputs in [\n","    (\"agentic\", outputs_agentic_rag),\n","    (\"standard\", outputs_standard_rag),\n","]:\n","\n","    results[system_type][\"eval_score_LLM_judge_int\"] = (\n","        results[system_type][\"eval_score_LLM_judge\"].fillna(DEFAULT_SCORE).apply(fill_score)\n","    )\n","    results[system_type][\"eval_score_LLM_judge_int\"] = (results[system_type][\"eval_score_LLM_judge_int\"] - 1) / 2\n","\n","    print(\n","        f\"Average score for {system_type} RAG: {results[system_type]['eval_score_LLM_judge_int'].mean()*100:.1f}%\"\n","    )"],"metadata":{"id":"NHzHIEGMyogL"},"execution_count":null,"outputs":[]}]}